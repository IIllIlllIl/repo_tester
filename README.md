# repo_tester
This is a Python program that generates and runs tests for a Python program file in a given GitHub repo. 
It takes the program's git repository and the relative path to the file as input. 
It generates a test file containing one test method per source-code method in the given file.

## Install
Clone this repo and install all dependencies.
```
git clone https://github.com/IIllIlllIl/repo_tester.git

pip install -r requirement.txt
```
Then, this program uses LLM to generate test assertions. Therefore, to run this program, a llm server API is required.
We recommend to use Openai style API.

## Usage
To understand this program, We provide an example of this program. The program under test in the example is a python 
program "ciphers/base32.py" in the GitHub repo: https://github.com/TheAlgorithms/Python.git.

### Example
Before running the test, you should first set LLM API info ("based_url", "temperature", "model", "key")
in the configuration file with your own LLM API. In the example, we use the model, "deepseek-chat". 

Run the program:
```
export PYTHONPATH='.'

python src/cmd_tester.py -c "config/example.json"
```

If this program does not generate test cases, this is because the instability of LLM. 
Rerun the program for multiple times will solve this problem.

Some output generated by this program is listed in the "example/".

### Test other repositories
If you want to test another Python file, you should first change the configurations. In the configuration, you should 
change the repo info ("repo_owner", "repo_name", "branch", "token") and 
LLM API info ("based_url", "temperature", "model", "key").

Then, you have to make sure all dependencies of the GitHub repo are installed. If some specific packages are expected 
to be imported in the test file, add them into the configuration file ("imports"). 
Usually, the attribute "import" is a empty list, "[]".

A configuration file contains:
- repo_owner: The owner of the GitHub repo.
- repo_name: The name of the GitHub repo.
- branch: The targeted branch in the GitHub repo.
- token: GitHub token.
- output: The local path where the repo should be cloned.
- based_url: The based URL of the LLM API.
- model: The name of the LLM model.
- imports: The models should be additionally imported in the test file.
- temperature: The temperature input for the LLM model.

Run this program with your own configuration:
```
python src/cmd_tester.py - c <path/to/config> -m <max_generation>
```

## Modification
If you want to write your own program based on this program, here are some detailed designs for better understanding.

### Files
Here list of the model files involved in the program flow:

- cmd_tester: It deals with the cmd input and calls other models accordingly.
- config: It reads attributes from the configuration JSON file.
- reader: It deals with files, including reading, writing, creating, and deleting.
- get_repo: It gets the file content of the file under test, and clones the GitHub repo.
- file_date: It analyzes the file content, extracts methods, and functions, and creates LLM prompts.
- response: It deals with the LLM outputs and generate test functions.
- model: It calls the LLM API to get test cases.
- dependency: It extracts imported packages from the original file.
- build file: It creates a test file with test functions.

### Flow
By running the cmd_tester, it will
1) process the cmd args
2) get the repo file and clone the repo
3) generate LLM prompts according to the file
4) call LLM API to generate test cases
5) build a test file
6) run pytest and report

### Prompting
This program currently provides two options for prompting.  
The Python method "prompting" in "file_data" builds the prompt.

- input_format: choose to provide source code or AST
- cot: use the chain of thought technique or not

If any improvement is expected, you can modify the method "prompting" in model "file_date":
```python
def prompting(self, model_name, input_format="text", cot=False):
    messages = []
    for m in self.methods:
        prompt = """Generate Python unit test code strictly following these requirements:
1. **Output Format**: Wrap the entire code in a Markdown Python code block (```python ... ```)
2. **Testing Framework**: Use pytest (not unittest) and only import pytest
3. **Test Organization**:
   - Structure tests as ONE function (no test classes)
   - Group by scenario in one function:
     ```python
     def test_{function_name}:
        # Normal cases
        ...
        # Edge/boundary cases
        ...
        # Error/exception cases
        ...
     ```
4. **Type Safety**:
   - All test parameters must match the function's type annotations
   - Validate type errors using `pytest.raises(TypeError)`
   - Include parameterized tests where appropriate
5. **Complete Structure**:
   ```python
   # Function under test (include definition)
   def target_function(...): ...
   
   # ===== Test cases  =====
   import pytest
   
   def test_{function_name}():
        # Normal cases
        ...
        # Edge/boundary cases
        ...
        # Error/exception cases
        ...
6. Purity: Output ONLY the Markdown code block with no additional text."""
        prompt += f"Function to test in model {model_name}:"
        if input_format == "ast":
            prompt += m['ast']
        else:
            prompt += m['text']
        if cot:
            prompt += "Let's think step by step:"
        messages.append(prompt)
    return messages
```